# LLM Configuration
LLM_PROVIDER=openai
LLM_MODEL=GPT-4o-mini
LLM_API_BASE_URL=https://aiportalapi.stu-platform.live/jpe #Just use for ollama only

# LLM Models Configuration
OPENAI_API_KEY=

# Embedding Configuration
EMBEDDING_PROVIDER=openai
EMBEDDING_MODEL=text-embedding-3-small
EMBEDDING_BATCH_SIZE=50
MAX_TOKENS_PER_BATCH=250000
EMBEDDING_API_KEY=

# Vector Database Configuration
DATABASE_TYPE=chroma #This configuration to allow switch vector store between chroma and pinecone
CHROMA_HOST=localhost
CHROMA_PORT=8000
CHROMA_COLLECTION_NAME=knowledge-base-graph

PINECONE_API_KEY=
PINECONE_API_BASE_URL=https://knowledge-base-graph-y99yepy.svc.aped-4627-b74a.pinecone.io
PINECONE_COLLECTION_NAME=knowledge-base-graph

# GitHub Configuration
GITHUB_TOKEN=
GITHUB_FILE_EXTENSIONS=[".cs",".csproj",".py",".php",".js",".jsx",".ts",".tsx",".html",".cshtml",".md", ".txt",".json",".yml",".yaml",".csv","dockerfile", ".config",".sh",".bash"]

# Application Configuration
APP_ENV=development #production
LOG_LEVEL=INFO #DEBUG
CHUNK_SIZE=1000
CHUNK_OVERLAP=200
MAX_TOKENS=4000
TEMPERATURE=0.7

# LangGraph Workflow Configuration
WORKFLOW_STATE_PERSISTENCE=true
WORKFLOW_RETRY_ATTEMPTS=3
WORKFLOW_RETRY_DELAY_SECONDS=5
WORKFLOW_TIMEOUT_SECONDS=3600
WORKFLOW_PARALLEL_REPOS=2
WORKFLOW_STATE_BACKEND=memory #database for production

# LangChain Configuration  
LANGCHAIN_TRACING=true #true for debugging
LANGCHAIN_API_KEY= #optional for LangSmith tracing
LANGCHAIN_PROJECT=knowledge-graph-agent #optional project name for LangSmith

# Web Configuration
API_BASE_URL=http://localhost:8000/api/v1

# Graph Database Configuration
ENABLE_GRAPH_FEATURES=false
GRAPH_STORE_TYPE=memgraph
GRAPH_STORE_URL=bolt://localhost:7687
GRAPH_STORE_USER= #optional, if authentication is required
GRAPH_STORE_PASSWORD= #optional, if authentication is required
GRAPH_STORE_DATABASE=knowledge_graph #optional, if a specific database is used

